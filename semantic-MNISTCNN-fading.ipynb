{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d99a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import ssl\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f6cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35a73d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 200\n",
    "\n",
    "# training SNR\n",
    "eb_n0 = 10\n",
    "channel_snr = 10\n",
    "rate = 1 # code rate \n",
    "channel_type = 'fading'\n",
    "\n",
    "\n",
    "#trn, tst = utils.get_mnist()\n",
    "\n",
    "# ##### KSG MI estimator #######################\n",
    "# settings = {'kraskov_k': 3}\n",
    "# gpu_est = est.OpenCLKraskovMI(settings = settings)\n",
    "# # usage: MI = estimator.estimate(var1, var2)\n",
    "\n",
    "##### setting for communication channel ######\n",
    "\n",
    "def getnoisevariance(SNR,rate,P=1):\n",
    "    # the SNR in args[0] is actually EbN0\n",
    "    snrdB = SNR + 10*np.log10(rate)\n",
    "    snr = 10.0**(snrdB/10.0)\n",
    "    #P_avg = 1\n",
    "    N0 = P/snr\n",
    "    return (N0/2)\n",
    "\n",
    "noise_var = getnoisevariance(eb_n0,rate,P=1)\n",
    "\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "label_permutation = np.arange(10)\n",
    "LABEL_FIRST_HALF = label_permutation[:5]\n",
    "label2_list = np.zeros((6,5))\n",
    "for i in range(6):\n",
    "    label2_list[i][:] = label_permutation[i:i+5]\n",
    "#LABEL_SECOND_HALF = [5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0d2cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalization to 1\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)) / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)) / 255\n",
    "# x_train = (x_train / 255.0)\n",
    "# x_test = (x_test / 255.0)\n",
    "# reshape to 1d\n",
    "# x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "# x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# save: original classification\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b848d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.where(np.in1d(y_train, LABEL_FIRST_HALF))\n",
    "test_filter =  np.where(np.in1d(y_test, LABEL_FIRST_HALF))\n",
    "\n",
    "x_1train = x_train[train_filter]\n",
    "Y_1train = Y_train[train_filter]\n",
    "x_1test = x_test[test_filter]\n",
    "Y_1test = Y_test[test_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7cb5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalized Input before \n",
    "# class GaussianNoiseLayer(layers.Layer):\n",
    "#     def __init__(self, stddev, **kwargs):\n",
    "#         super(GaussianNoiseLayer, self).__init__(**kwargs)\n",
    "#         self.stddev = stddev\n",
    "    \n",
    "#     def call(self, inputs, training=None):\n",
    "#         if training:\n",
    "#             noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=self.stddev)\n",
    "#             return inputs + noise\n",
    "#         else:\n",
    "#             noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=self.stddev)\n",
    "#             return inputs + noise\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super(GaussianNoiseLayer, self).get_config()\n",
    "#         config.update({'noise_var': self.stddev})\n",
    "#         return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdc384bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fading channel\n",
    "\n",
    "def fading(x, stddev, h=None):\n",
    "    \"\"\"Implements the fading channel with multiplicative fading and\n",
    "    additive white gaussian noise.\n",
    "    Args:\n",
    "        x: channel input symbols\n",
    "        stddev: standard deviation of noise\n",
    "    Returns:\n",
    "        y: noisy channel output symbols\n",
    "    \"\"\"\n",
    "    # channel gain\n",
    "    if h is None:\n",
    "        h = tf.complex(\n",
    "            tf.random.normal([tf.shape(x)[0], 1], 0, 1 / np.sqrt(2)),\n",
    "            tf.random.normal([tf.shape(x)[0], 1], 0, 1 / np.sqrt(2)),\n",
    "        )\n",
    "\n",
    "    # additive white gaussian noise\n",
    "    awgn = tf.complex(\n",
    "        tf.random.normal(tf.shape(x), 0, 1 / np.sqrt(2)),\n",
    "        tf.random.normal(tf.shape(x), 0, 1 / np.sqrt(2)),\n",
    "    )\n",
    "\n",
    "    return (h * x + stddev * awgn), h\n",
    "\n",
    "class Channel(layers.Layer):\n",
    "\n",
    "#cite D-JSCC\n",
    "\n",
    "    def __init__(self, channel_type, channel_snr, name=\"channel\", **kwargs):\n",
    "        super(Channel, self).__init__(name=name, **kwargs)\n",
    "        self.channel_type = channel_type\n",
    "        self.channel_snr = channel_snr\n",
    "\n",
    "    def call(self, inputs):\n",
    "        (encoded_img, prev_h) = inputs\n",
    "        inter_shape = tf.shape(encoded_img)\n",
    "        # reshape array to [-1, dim_z]\n",
    "        z = layers.Flatten()(encoded_img)\n",
    "        # convert from snr to std\n",
    "        # print(\"channel_snr: {}\".format(self.channel_snr))\n",
    "        noise_stddev = np.sqrt(10 ** (-self.channel_snr / 10))\n",
    "\n",
    "        # Add channel noise\n",
    "        if self.channel_type == \"awgn\":\n",
    "            dim_z = tf.shape(z)[1]\n",
    "            # normalize latent vector so that the average power is 1\n",
    "            z_in = tf.sqrt(tf.cast(dim_z, dtype=tf.float32)) * tf.nn.l2_normalize(\n",
    "                z, axis=1\n",
    "            )\n",
    "            z_out = real_awgn(z_in, noise_stddev)\n",
    "            h = tf.ones_like(z_in)  # h just makes sense on fading channels\n",
    "\n",
    "        elif self.channel_type == \"fading\":\n",
    "            dim_z = tf.shape(z)[1] // 2\n",
    "            # convert z to complex representation\n",
    "            z_in = tf.complex(z[:, :dim_z], z[:, dim_z:])\n",
    "            # normalize the latent vector so that the average power is 1\n",
    "            z_norm = tf.reduce_sum(\n",
    "                tf.math.real(z_in * tf.math.conj(z_in)), axis=1, keepdims=True\n",
    "            )\n",
    "            z_in = z_in * tf.complex(\n",
    "                tf.sqrt(tf.cast(dim_z, dtype=tf.float32) / z_norm), 0.0\n",
    "            )\n",
    "            z_out, h = fading(z_in, noise_stddev, prev_h)\n",
    "            # convert back to real\n",
    "            z_out = tf.concat([tf.math.real(z_out), tf.math.imag(z_out)], 1)\n",
    "\n",
    "        # convert signal back to intermediate shape\n",
    "        z_out = tf.reshape(z_out, inter_shape)\n",
    "\n",
    "        return z_out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b21aee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, display_frequency=10):\n",
    "        super(CustomMetricCallback, self).__init__()\n",
    "        self.display_frequency = display_frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.display_frequency == 0:\n",
    "            metrics_str = \" - \".join([f\"{metric_name}: {value:.4f}\" for metric_name, value in logs.items()])\n",
    "            print(f\"Epoch {epoch}/{self.params['epochs']} - {metrics_str}\")\n",
    "\n",
    "            \n",
    "callback = CustomMetricCallback(display_frequency=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "610f52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_report(epoch):\n",
    "#     if epoch < 20:       # Log for all first 20 epochs\n",
    "#         return True\n",
    "    if epoch < 100:    # Then for every 5th epoch\n",
    "        return (epoch % 5 == 0)\n",
    "    elif epoch < 200:    # Then every 10th\n",
    "        return (epoch % 10 == 0)\n",
    "    else:                # Then every 100th\n",
    "        return (epoch % 100 == 0)\n",
    "    \n",
    "    \n",
    "def train_first_stage(lambda_val):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    prev_chn_gain = None\n",
    "\n",
    "        ############ Model Structure ###############################\n",
    "\n",
    "    input_layer  = tf.keras.layers.Input(shape=x_train[0].shape)\n",
    "    encoder_1 = layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(input_layer)\n",
    "    maxpool_1 = layers.MaxPooling2D((2, 2))(encoder_1)\n",
    "    encoder_2 = layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)(encoder_1)\n",
    "    maxpool_2 = layers.MaxPooling2D((2, 2))(encoder_2)\n",
    "\n",
    "    normalized_x = tf.keras.layers.Lambda(lambda x: K.tanh(x))(encoder_2)\n",
    "    fadingchannel = Channel(channel_type, channel_snr, name=\"channel_output\")\n",
    "    noise_layer, chn_gain = fadingchannel((normalized_x, prev_chn_gain))\n",
    "    #noise_layer = GaussianNoiseLayer(stddev = sd)(normalized_x)\n",
    "\n",
    "    ## first loss: classification \n",
    "#     CE_decoder_1 =  layers.Conv2D(32, (3, 3), activation='relu')(noise_layer)\n",
    "#     CE_maxpool_1 = layers.MaxPooling2D((2, 2))(CE_decoder_1)\n",
    "    # CE_decoder_2 =  layers.Conv2D(32, (3, 3), activation='relu')(CE_maxpool_1)\n",
    "    # CE_maxpool_2 = layers.MaxPooling2D((2, 2))(CE_decoder_2)\n",
    "    flatten_1 = layers.Flatten()(noise_layer)\n",
    "    CE_dense_2 = layers.Dense(40, activation='relu')(flatten_1)\n",
    "    CE_output = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_dense_2)\n",
    "    #CE_output = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_decoder_2)\n",
    "\n",
    "    ## second loss: reconstruction\n",
    "    mse_1 = layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same')(noise_layer)\n",
    "    mse_2 = layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same')(mse_1)\n",
    "    mse_output = layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same',name='mse')(mse_2)\n",
    "\n",
    "    #    mse_decoder_1 = tf.keras.layers.Dense(256, activation='relu')(noise_layer)\n",
    "    #    mse_output = tf.keras.layers.Dense(x_train.shape[1],activation = 'sigmoid',name='mse')(mse_decoder_1)  \n",
    "\n",
    "    model = tf.keras.Model(inputs = input_layer, outputs = [CE_output,mse_output])\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.8)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss = {'CE' : 'categorical_crossentropy', \n",
    "                          'mse' : 'mse'},\n",
    "                  metrics = {'CE' : 'accuracy', \n",
    "                             'mse': tf.keras.metrics.RootMeanSquaredError()},\n",
    "                  loss_weights=[1, lambda_val])\n",
    "\n",
    "\n",
    "    SNR = eb_n0\n",
    "    #     print(\"current snr: \",SNR)\n",
    "    # #     print(\"====================\")\n",
    "    #     reporter = getMIOutput(trn=x_1train, \n",
    "    #                                tst=x_1test, \n",
    "    #                                snr=SNR,\n",
    "    #                               do_save_func=do_report)\n",
    "\n",
    "    history = model.fit(x=x_1train, y=(Y_1train,x_1train),\n",
    "                        batch_size=128,\n",
    "                        epochs=100,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_1test, (Y_1test,x_1test)))\n",
    "                       #callbacks = [callback])\n",
    "                        #callbacks=[reporter,])\n",
    "    print(\"acc:\", history.history['val_CE_accuracy'][-1]) \n",
    "\n",
    "\n",
    "#     ep_last = len(reporter.datalist) -1\n",
    "#     # Z_data = reporter.datalist[EPOCH]['data']['activity_tst'][3][:10000]\n",
    "#     ZHAT_data = reporter.datalist[ep_last]['data']['activity_tst'][4][:SAMPLE_NUM]\n",
    "#     x_selected = funcs.normalize_data_new(x_1train[:SAMPLE_NUM],C=10)[0]\n",
    "#     print(\"Current I(Z_hat; X) is: \",gpu_est.estimate(funcs.normalize_data_new(ZHAT_data,C=4)[0],x_selected))\n",
    "\n",
    "  \n",
    "    ################### secpmd stage #################\n",
    "    print(\"stage 2!\")\n",
    "   \n",
    "    train_filter = np.where(np.in1d(y_train, LABEL_SECOND_HALF))\n",
    "    test_filter =  np.where(np.in1d(y_test, LABEL_SECOND_HALF))\n",
    "\n",
    "    x_retrain = x_train[train_filter]\n",
    "    x_retest = x_test[test_filter]\n",
    "    Y_retrain = Y_train[train_filter]\n",
    "    Y_retest = Y_test[test_filter]\n",
    "\n",
    "    flatten_2 = layers.Flatten()(noise_layer)\n",
    "    CE_dense_3 = layers.Dense(40, activation='relu')(flatten_2)\n",
    "    CE_output1 = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_dense_2)\n",
    "    #     CE_decoder_3 = tf.keras.layers.Dense(64, activation='relu')(noise_layer)\n",
    "    #     CE_decoder_4 = tf.keras.layers.Dense(64, activation='relu')(CE_decoder_3)\n",
    "    #     CE_output1 = tf.keras.layers.Dense(10, activation='softmax', name='CE')(CE_decoder_4)\n",
    "\n",
    "    reconstructed_model = tf.keras.models.Model(inputs = input_layer, outputs = [CE_output1,mse_output])\n",
    "\n",
    "    reconstructed_model.layers[1].trainable = False\n",
    "    reconstructed_model.layers[2].trainable = False\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.8)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "    reconstructed_model.compile(optimizer=opt, \n",
    "                loss = {'CE' : 'categorical_crossentropy', \n",
    "                          'mse' : 'mse'\n",
    "                         },              loss_weights=[1, 0],\n",
    "                  metrics = {'CE' : 'accuracy', \n",
    "                             'mse': tf.keras.metrics.RootMeanSquaredError()\n",
    "                           }\n",
    "                 )\n",
    "\n",
    "    #     reporter_2 = getMIOutput(trn=x_retrain, \n",
    "    #                                tst=Y_retrain, \n",
    "    #                                snr=SNR,\n",
    "    #                               do_save_func=do_report)\n",
    "\n",
    "    history1 = reconstructed_model.fit(x=x_retrain, y=(Y_retrain,x_retrain),\n",
    "                        batch_size=256,\n",
    "                        epochs=100,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_retest, (Y_retest,x_retest)))\n",
    "                        # callbacks=[callback])\n",
    "    print(\"acc:\", history1.history['val_CE_accuracy'][-1]) \n",
    "    return history.history['val_CE_accuracy'][-1], history1.history['val_CE_accuracy'][-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55d4bff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 10:34:31.363129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2024-02-22 10:34:31.621891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:655] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-02-22 10:34:31.888633: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f9474312ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-22 10:34:31.888678: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-02-22 10:34:32.024950: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9861840605735779\n",
      "stage 2!\n",
      "acc: 0.9642049074172974\n",
      "lambda:  1\n",
      "acc: 0.9842382073402405\n",
      "stage 2!\n",
      "acc: 0.9642049074172974\n",
      "lambda:  3\n",
      "acc: 0.9842382073402405\n",
      "stage 2!\n",
      "acc: 0.9605019688606262\n",
      "lambda:  10\n",
      "acc: 0.9828760623931885\n",
      "stage 2!\n",
      "acc: 0.9602962136268616\n",
      "current overlap is:  1\n",
      "lambda:  0\n",
      "acc: 0.9861840605735779\n",
      "stage 2!\n",
      "acc: 0.974555253982544\n",
      "lambda:  1\n",
      "acc: 0.9877408146858215\n",
      "stage 2!\n",
      "acc: 0.9760032892227173\n",
      "lambda:  3\n",
      "acc: 0.984627366065979\n",
      "stage 2!\n",
      "acc: 0.970417857170105\n",
      "lambda:  10\n",
      "acc: 0.9782058596611023\n",
      "stage 2!\n",
      "acc: 0.9635912179946899\n",
      "current overlap is:  2\n",
      "lambda:  0\n",
      "acc: 0.9875462055206299\n",
      "stage 2!\n",
      "acc: 0.9743326306343079\n",
      "lambda:  1\n",
      "acc: 0.9879354238510132\n",
      "stage 2!\n",
      "acc: 0.9755646586418152\n",
      "lambda:  3\n",
      "acc: 0.9789842367172241\n",
      "stage 2!\n",
      "acc: 0.9720739126205444\n",
      "lambda:  10\n",
      "acc: 0.9813193082809448\n",
      "stage 2!\n",
      "acc: 0.9708418846130371\n",
      "current overlap is:  3\n",
      "lambda:  0\n",
      "acc: 0.9838489890098572\n",
      "stage 2!\n",
      "acc: 0.9772260785102844\n",
      "lambda:  1\n",
      "acc: 0.9867678284645081\n",
      "stage 2!\n",
      "acc: 0.9759950637817383\n",
      "lambda:  3\n",
      "acc: 0.9865732789039612\n",
      "stage 2!\n",
      "acc: 0.9743537306785583\n",
      "lambda:  10\n",
      "acc: 0.9813193082809448\n",
      "stage 2!\n",
      "acc: 0.9647107124328613\n",
      "current overlap is:  4\n",
      "lambda:  0\n",
      "acc: 0.9856002926826477\n",
      "stage 2!\n",
      "acc: 0.9802019596099854\n",
      "lambda:  1\n",
      "acc: 0.9836543798446655\n",
      "stage 2!\n",
      "acc: 0.9776281714439392\n",
      "lambda:  3\n",
      "acc: 0.9834598302841187\n",
      "stage 2!\n",
      "acc: 0.9807958602905273\n",
      "lambda:  10\n",
      "acc: 0.9807355403900146\n",
      "stage 2!\n",
      "acc: 0.9764403104782104\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.9848219752311707\n",
      "stage 2!\n",
      "acc: 0.9840435981750488\n",
      "lambda:  1\n",
      "acc: 0.9856002926826477\n",
      "stage 2!\n",
      "acc: 0.985989511013031\n",
      "lambda:  3\n",
      "acc: 0.9834598302841187\n",
      "stage 2!\n",
      "acc: 0.9815139174461365\n",
      "lambda:  10\n",
      "acc: 0.9813193082809448\n",
      "stage 2!\n",
      "acc: 0.983265221118927\n"
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,1,3,10]\n",
    "acc1_list = []\n",
    "acc2_list = []\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    LABEL_SECOND_HALF = label2_list[5-i]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        acc1, acc2 = train_first_stage(lambda_val)\n",
    "        acc1_list.append(acc1)\n",
    "        acc2_list.append(acc2)\n",
    "        #train_second_stage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd1b315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97576074 0.9758976  0.97327621 0.96985759]\n"
     ]
    }
   ],
   "source": [
    "#print(np.reshape(acc2_list,(6,-1)))\n",
    "print(np.mean(np.reshape(acc2_list,(6,-1)),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18319bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave = []\n",
    "for i in range(1000):\n",
    "    x= np.random.normal(0,0.5) \n",
    "    y= np.random.normal(0,0.5) \n",
    "    #print(x)\n",
    "\n",
    "    ave.append(np.sqrt(x**2 + y**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd37ca37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.451984914322634"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(1/np.array(ave))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58ff1c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(ave)>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45aa433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fcc78df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83920002, 0.85339999, 0.84380001, 0.83960003, 0.85000002,\n",
       "        0.8484    , 0.86400002, 0.85839999],\n",
       "       [0.79500002, 0.79939997, 0.80779999, 0.78799999, 0.80900002,\n",
       "        0.80800003, 0.7974    , 0.80400002],\n",
       "       [0.68919998, 0.66939998, 0.6796    , 0.69300002, 0.67720002,\n",
       "        0.68300003, 0.68220001, 0.69      ],\n",
       "       [0.63480002, 0.63239998, 0.6318    , 0.63620001, 0.6462    ,\n",
       "        0.62639999, 0.6462    , 0.653     ],\n",
       "       [0.70060003, 0.69440001, 0.68519998, 0.70880002, 0.70060003,\n",
       "        0.6904    , 0.6918    , 0.69980001],\n",
       "       [0.76160002, 0.76959997, 0.76959997, 0.76160002, 0.77640003,\n",
       "        0.76840001, 0.76440001, 0.77020001]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(acc2_list,(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c4df6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1: [0.7185999751091003, 0.7432000041007996, 0.7235999703407288, 0.7214000225067139, 0.728600025177002, 0.7211999893188477, 0.7229999899864197, 0.7287999987602234, 0.7129999995231628, 0.7084000110626221, 0.7128000259399414, 0.7134000062942505, 0.7185999751091003, 0.7369999885559082, 0.7264000177383423, 0.7347999811172485, 0.7164000272750854, 0.7103999853134155, 0.723800003528595, 0.7139999866485596, 0.7251999974250793, 0.7251999974250793, 0.7253999710083008, 0.7287999987602234, 0.7006000280380249, 0.7088000178337097, 0.715399980545044, 0.725600004196167, 0.7282000184059143, 0.7269999980926514, 0.7229999899864197, 0.7192000150680542, 0.7232000231742859, 0.7113999724388123, 0.7235999703407288, 0.728600025177002, 0.7289999723434448, 0.7354000210762024, 0.730400025844574, 0.7319999933242798, 0.7206000089645386, 0.7211999893188477, 0.7218000292778015, 0.717199981212616, 0.715399980545044, 0.7325999736785889, 0.7283999919891357, 0.7210000157356262]\n",
      "Array 2: [0.8004000186920166, 0.8220000267028809, 0.7991999983787537, 0.807200014591217, 0.8181999921798706, 0.8191999793052673, 0.8126000165939331, 0.8230000138282776, 0.7728000283241272, 0.7734000086784363, 0.767799973487854, 0.774399995803833, 0.7716000080108643, 0.7843999862670898, 0.7817999720573425, 0.7937999963760376, 0.7748000025749207, 0.7879999876022339, 0.7698000073432922, 0.7680000066757202, 0.7825999855995178, 0.775600016117096, 0.7850000262260437, 0.7936000227928162, 0.7505999803543091, 0.7563999891281128, 0.7681999802589417, 0.7684000134468079, 0.7739999890327454, 0.7703999876976013, 0.7789999842643738, 0.7644000053405762, 0.7843999862670898, 0.7811999917030334, 0.7821999788284302, 0.7829999923706055, 0.7883999943733215, 0.7888000011444092, 0.7806000113487244, 0.7960000038146973, 0.728600025177002, 0.7271999716758728, 0.7324000000953674, 0.7300000190734863, 0.7261999845504761, 0.73580002784729, 0.7423999905586243, 0.7404000163078308]\n"
     ]
    }
   ],
   "source": [
    "### please given data ###\n",
    "\n",
    "lines = data.strip().split('\\n')\n",
    "array1 = []\n",
    "array2 = []\n",
    "is_stage2_next = False\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if \"stage 2!\" in lines[i]:\n",
    "        is_stage2_next = True\n",
    "        continue\n",
    "    \n",
    "    if lines[i].startswith(\"acc:\"):\n",
    "        acc_value = float(lines[i].split(\":\")[1])\n",
    "        if is_stage2_next:\n",
    "            array2.append(acc_value)\n",
    "            is_stage2_next = False\n",
    "        else:\n",
    "            array1.append(acc_value)\n",
    "\n",
    "print(\"Array 1:\", array1)\n",
    "print(\"Array 2:\", array2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763d292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(array1,(6,-1))\n",
    "#print(np.reshape(array2,(6,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ad5937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 13:31:20.039462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78836 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2023-08-11 13:31:23.317045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2023-08-11 13:31:23.562262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:655] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-11 13:31:23.656432: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7faf088588c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-11 13:31:23.656474: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2023-08-11 13:31:23.735796: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7039999961853027\n",
      "stage 2!\n",
      "acc: 0.7901999950408936\n",
      "lambda:  0.2\n",
      "acc: 0.7228000164031982\n",
      "stage 2!\n",
      "acc: 0.8159999847412109\n",
      "lambda:  0.5\n",
      "acc: 0.7152000069618225\n",
      "stage 2!\n",
      "acc: 0.8037999868392944\n",
      "lambda:  1\n",
      "acc: 0.7235999703407288\n",
      "stage 2!\n",
      "acc: 0.8091999888420105\n",
      "lambda:  3\n",
      "acc: 0.7178000211715698\n",
      "stage 2!\n",
      "acc: 0.8068000078201294\n",
      "lambda:  5\n",
      "acc: 0.7300000190734863\n",
      "stage 2!\n",
      "acc: 0.8203999996185303\n",
      "lambda:  10\n",
      "acc: 0.7332000136375427\n",
      "stage 2!\n",
      "acc: 0.8252000212669373\n",
      "lambda:  15\n",
      "acc: 0.7215999960899353\n",
      "stage 2!\n",
      "acc: 0.8154000043869019\n",
      "current overlap is:  1\n",
      "lambda:  0\n",
      "acc: 0.7175999879837036\n",
      "stage 2!\n",
      "acc: 0.7746000289916992\n",
      "lambda:  0.2\n",
      "acc: 0.7203999757766724\n",
      "stage 2!\n",
      "acc: 0.7853999733924866\n",
      "lambda:  0.5\n",
      "acc: 0.728600025177002\n",
      "stage 2!\n",
      "acc: 0.7770000100135803\n",
      "lambda:  1\n",
      "acc: 0.7184000015258789\n",
      "stage 2!\n",
      "acc: 0.7752000093460083\n",
      "lambda:  3\n",
      "acc: 0.7283999919891357\n",
      "stage 2!\n",
      "acc: 0.775600016117096\n",
      "lambda:  5\n",
      "acc: 0.7342000007629395\n",
      "stage 2!\n",
      "acc: 0.7832000255584717\n",
      "lambda:  10\n",
      "acc: 0.7332000136375427\n",
      "stage 2!\n",
      "acc: 0.7942000031471252\n",
      "lambda:  15\n",
      "acc: 0.7260000109672546\n",
      "stage 2!\n",
      "acc: 0.7807999849319458\n",
      "current overlap is:  2\n",
      "lambda:  0\n",
      "acc: 0.734000027179718\n",
      "stage 2!\n",
      "acc: 0.7910000085830688\n",
      "lambda:  0.2\n",
      "acc: 0.7167999744415283\n",
      "stage 2!\n",
      "acc: 0.7803999781608582\n",
      "lambda:  0.5\n",
      "acc: 0.7196000218391418\n",
      "stage 2!\n",
      "acc: 0.7653999924659729\n",
      "lambda:  1\n",
      "acc: 0.7265999913215637\n",
      "stage 2!\n",
      "acc: 0.7868000268936157\n",
      "lambda:  3\n",
      "acc: 0.7311999797821045\n",
      "stage 2!\n",
      "acc: 0.7914000153541565\n",
      "lambda:  5\n",
      "acc: 0.7336000204086304\n",
      "stage 2!\n",
      "acc: 0.7924000024795532\n",
      "lambda:  10\n",
      "acc: 0.7372000217437744\n",
      "stage 2!\n",
      "acc: 0.7997999787330627\n",
      "lambda:  15\n",
      "acc: 0.7301999926567078\n",
      "stage 2!\n",
      "acc: 0.7961999773979187\n",
      "current overlap is:  3\n",
      "lambda:  0\n",
      "acc: 0.7143999934196472\n",
      "stage 2!\n",
      "acc: 0.7558000087738037\n",
      "lambda:  0.2\n",
      "acc: 0.7129999995231628\n",
      "stage 2!\n",
      "acc: 0.7552000284194946\n",
      "lambda:  0.5\n",
      "acc: 0.7188000082969666\n",
      "stage 2!\n",
      "acc: 0.7567999958992004\n",
      "lambda:  1\n",
      "acc: 0.7124000191688538\n",
      "stage 2!\n",
      "acc: 0.7552000284194946\n",
      "lambda:  3\n",
      "acc: 0.7146000266075134\n",
      "stage 2!\n",
      "acc: 0.7667999863624573\n",
      "lambda:  5\n",
      "acc: 0.7242000102996826\n",
      "stage 2!\n",
      "acc: 0.7746000289916992\n",
      "lambda:  10\n",
      "acc: 0.7343999743461609\n",
      "stage 2!\n",
      "acc: 0.7835999727249146\n",
      "lambda:  15\n",
      "acc: 0.7293999791145325\n",
      "stage 2!\n",
      "acc: 0.776199996471405\n",
      "current overlap is:  4\n",
      "lambda:  0\n",
      "acc: 0.7142000198364258\n",
      "stage 2!\n",
      "acc: 0.7688000202178955\n",
      "lambda:  0.2\n",
      "acc: 0.7167999744415283\n",
      "stage 2!\n",
      "acc: 0.7897999882698059\n",
      "lambda:  0.5\n",
      "acc: 0.7264000177383423\n",
      "stage 2!\n",
      "acc: 0.7775999903678894\n",
      "lambda:  1\n",
      "acc: 0.7233999967575073\n",
      "stage 2!\n",
      "acc: 0.7770000100135803\n",
      "lambda:  3\n",
      "acc: 0.7260000109672546\n",
      "stage 2!\n",
      "acc: 0.7811999917030334\n",
      "lambda:  5\n",
      "acc: 0.7182000279426575\n",
      "stage 2!\n",
      "acc: 0.7839999794960022\n",
      "lambda:  10\n",
      "acc: 0.7251999974250793\n",
      "stage 2!\n",
      "acc: 0.7870000004768372\n",
      "lambda:  15\n",
      "acc: 0.7287999987602234\n",
      "stage 2!\n",
      "acc: 0.7911999821662903\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.7052000164985657\n",
      "stage 2!\n",
      "acc: 0.7210000157356262\n",
      "lambda:  0.2\n",
      "acc: 0.72079998254776\n",
      "stage 2!\n",
      "acc: 0.745199978351593\n",
      "lambda:  0.5\n",
      "acc: 0.7233999967575073\n",
      "stage 2!\n",
      "acc: 0.7310000061988831\n",
      "lambda:  1\n",
      "acc: 0.7175999879837036\n",
      "stage 2!\n",
      "acc: 0.7287999987602234\n",
      "lambda:  3\n",
      "acc: 0.7314000129699707\n",
      "stage 2!\n",
      "acc: 0.7378000020980835\n",
      "lambda:  5\n",
      "acc: 0.7110000252723694\n",
      "stage 2!\n",
      "acc: 0.739799976348877\n",
      "lambda:  10\n",
      "acc: 0.7354000210762024\n",
      "stage 2!\n",
      "acc: 0.7400000095367432\n",
      "lambda:  15\n",
      "acc: 0.7289999723434448\n",
      "stage 2!\n",
      "acc: 0.7505999803543091\n"
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,0.2,0.5,1,3,5,10,15]\n",
    "acc1_list = []\n",
    "acc2_list = []\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    LABEL_SECOND_HALF = label2_list[5-i]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        acc1, acc2 = train_first_stage(lambda_val)\n",
    "        acc1_list.append(acc1)\n",
    "        acc2_list.append(acc2)\n",
    "        #train_second_stage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169755b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 04:55:05.857821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1131 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2023-08-12 04:55:08.293360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2023-08-12 04:55:08.483354: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 891.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:08.486133: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 891.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:08.528721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:655] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-12 04:55:08.615359: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x55c2972e6b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-12 04:55:08.615419: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2023-08-12 04:55:08.623795: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 320.06MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:08.689556: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-08-12 04:55:08.761635: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 265.03MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:08.761715: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 265.03MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:09.031423: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 560.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:09.031502: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 560.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:09.195946: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 284.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:09.509104: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 891.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:09.511485: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 891.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-12 04:55:22.732681: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 96.00MiB (rounded to 100663296)requested by op gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-08-12 04:55:22.732735: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-08-12 04:55:22.732748: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 76, Chunks in use: 75. 19.0KiB allocated for chunks. 18.8KiB in use in bin. 1.4KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732755: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 2, Chunks in use: 2. 1.0KiB allocated for chunks. 1.0KiB in use in bin. 768B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732761: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 3, Chunks in use: 3. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 3.3KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732766: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 3, Chunks in use: 2. 6.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732770: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732776: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 20.5KiB allocated for chunks. 20.5KiB in use in bin. 20.2KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732781: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 25.8KiB allocated for chunks. 25.8KiB in use in bin. 20.0KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732786: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 7, Chunks in use: 7. 252.0KiB allocated for chunks. 252.0KiB in use in bin. 236.0KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732793: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 324.0KiB allocated for chunks. 324.0KiB in use in bin. 324.0KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732799: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 1, Chunks in use: 1. 195.5KiB allocated for chunks. 195.5KiB in use in bin. 195.3KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732803: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732809: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 976.8KiB allocated for chunks. 976.8KiB in use in bin. 976.6KiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732815: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732819: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732825: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 4, Chunks in use: 3. 21.11MiB allocated for chunks. 15.21MiB in use in bin. 11.00MiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732830: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732834: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732839: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 4, Chunks in use: 3. 197.19MiB allocated for chunks. 149.19MiB in use in bin. 149.19MiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732844: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732851: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 2. 325.42MiB allocated for chunks. 325.42MiB in use in bin. 192.00MiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732857: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 2. 585.94MiB allocated for chunks. 585.94MiB in use in bin. 585.94MiB client-requested in use in bin.\n",
      "2023-08-12 04:55:22.732862: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 96.00MiB was 64.00MiB, Chunk State: \n",
      "2023-08-12 04:55:22.732866: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 1186398208\n",
      "2023-08-12 04:55:22.732872: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000000 of size 1280 next 1\n",
      "2023-08-12 04:55:22.732876: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000500 of size 256 next 2\n",
      "2023-08-12 04:55:22.732880: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000600 of size 256 next 3\n",
      "2023-08-12 04:55:22.732884: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000700 of size 256 next 5\n",
      "2023-08-12 04:55:22.732888: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000800 of size 512 next 6\n",
      "2023-08-12 04:55:22.732892: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000a00 of size 256 next 4\n",
      "2023-08-12 04:55:22.732896: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000b00 of size 256 next 7\n",
      "2023-08-12 04:55:22.732899: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000c00 of size 256 next 12\n",
      "2023-08-12 04:55:22.732903: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000d00 of size 256 next 10\n",
      "2023-08-12 04:55:22.732907: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000e00 of size 256 next 11\n",
      "2023-08-12 04:55:22.732910: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000f00 of size 256 next 17\n",
      "2023-08-12 04:55:22.732914: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001000 of size 256 next 15\n",
      "2023-08-12 04:55:22.732917: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001100 of size 256 next 16\n",
      "2023-08-12 04:55:22.732921: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001200 of size 2048 next 22\n",
      "2023-08-12 04:55:22.732925: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001a00 of size 256 next 20\n",
      "2023-08-12 04:55:22.732928: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001b00 of size 256 next 21\n",
      "2023-08-12 04:55:22.732932: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001c00 of size 256 next 27\n",
      "2023-08-12 04:55:22.732936: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001d00 of size 256 next 25\n",
      "2023-08-12 04:55:22.732940: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001e00 of size 256 next 26\n",
      "2023-08-12 04:55:22.732943: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001f00 of size 256 next 31\n",
      "2023-08-12 04:55:22.732947: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002000 of size 256 next 32\n",
      "2023-08-12 04:55:22.732950: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002100 of size 256 next 34\n",
      "2023-08-12 04:55:22.732954: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002200 of size 256 next 35\n",
      "2023-08-12 04:55:22.732957: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002300 of size 256 next 33\n",
      "2023-08-12 04:55:22.732961: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002400 of size 256 next 36\n",
      "2023-08-12 04:55:22.732964: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002500 of size 256 next 39\n",
      "2023-08-12 04:55:22.732968: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002600 of size 256 next 40\n",
      "2023-08-12 04:55:22.732973: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002700 of size 256 next 41\n",
      "2023-08-12 04:55:22.732976: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002800 of size 256 next 42\n",
      "2023-08-12 04:55:22.732980: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002900 of size 256 next 43\n",
      "2023-08-12 04:55:22.732983: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002a00 of size 256 next 44\n",
      "2023-08-12 04:55:22.732987: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002b00 of size 256 next 45\n",
      "2023-08-12 04:55:22.732990: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002c00 of size 256 next 49\n",
      "2023-08-12 04:55:22.732994: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002d00 of size 256 next 37\n",
      "2023-08-12 04:55:22.732997: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002e00 of size 1280 next 38\n",
      "2023-08-12 04:55:22.733001: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003300 of size 256 next 50\n",
      "2023-08-12 04:55:22.733004: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003400 of size 256 next 51\n",
      "2023-08-12 04:55:22.733008: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003500 of size 256 next 52\n",
      "2023-08-12 04:55:22.733012: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003600 of size 256 next 53\n",
      "2023-08-12 04:55:22.733017: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003700 of size 256 next 54\n",
      "2023-08-12 04:55:22.733021: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003800 of size 512 next 56\n",
      "2023-08-12 04:55:22.733025: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003a00 of size 256 next 58\n",
      "2023-08-12 04:55:22.733028: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003b00 of size 256 next 59\n",
      "2023-08-12 04:55:22.733032: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003c00 of size 256 next 61\n",
      "2023-08-12 04:55:22.733036: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003d00 of size 2048 next 62\n",
      "2023-08-12 04:55:22.733039: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004500 of size 256 next 64\n",
      "2023-08-12 04:55:22.733043: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004600 of size 256 next 65\n",
      "2023-08-12 04:55:22.733047: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004700 of size 1280 next 66\n",
      "2023-08-12 04:55:22.733050: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004c00 of size 256 next 67\n",
      "2023-08-12 04:55:22.733054: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004d00 of size 256 next 68\n",
      "2023-08-12 04:55:22.733057: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004e00 of size 256 next 69\n",
      "2023-08-12 04:55:22.733061: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004f00 of size 256 next 70\n",
      "2023-08-12 04:55:22.733065: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005000 of size 256 next 71\n",
      "2023-08-12 04:55:22.733068: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005100 of size 256 next 72\n",
      "2023-08-12 04:55:22.733073: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005200 of size 256 next 73\n",
      "2023-08-12 04:55:22.733076: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005300 of size 256 next 74\n",
      "2023-08-12 04:55:22.733080: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005400 of size 256 next 75\n",
      "2023-08-12 04:55:22.733083: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005500 of size 256 next 76\n",
      "2023-08-12 04:55:22.733087: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005600 of size 256 next 77\n",
      "2023-08-12 04:55:22.733090: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005700 of size 256 next 78\n",
      "2023-08-12 04:55:22.733094: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005800 of size 256 next 79\n",
      "2023-08-12 04:55:22.733098: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005900 of size 256 next 109\n",
      "2023-08-12 04:55:22.733102: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005a00 of size 256 next 88\n",
      "2023-08-12 04:55:22.733106: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005b00 of size 256 next 8\n",
      "2023-08-12 04:55:22.733110: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005c00 of size 10496 next 9\n",
      "2023-08-12 04:55:22.733114: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc008500 of size 40960 next 29\n",
      "2023-08-12 04:55:22.733118: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc012500 of size 32768 next 19\n",
      "2023-08-12 04:55:22.733121: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc01a500 of size 36864 next 18\n",
      "2023-08-12 04:55:22.733125: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc023500 of size 10496 next 55\n",
      "2023-08-12 04:55:22.733129: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc025e00 of size 26368 next 30\n",
      "2023-08-12 04:55:22.733133: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc02c500 of size 36864 next 28\n",
      "2023-08-12 04:55:22.733136: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc035500 of size 36864 next 14\n",
      "2023-08-12 04:55:22.733140: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc03e500 of size 110592 next 13\n",
      "2023-08-12 04:55:22.733144: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc059500 of size 1000192 next 47\n",
      "2023-08-12 04:55:22.733148: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc14d800 of size 110592 next 57\n",
      "2023-08-12 04:55:22.733152: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc168800 of size 36864 next 60\n",
      "2023-08-12 04:55:22.733155: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc171800 of size 7240960 next 24\n",
      "2023-08-12 04:55:22.733159: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc859500 of size 4194304 next 23\n",
      "2023-08-12 04:55:22.733163: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bcc59500 of size 307200000 next 46\n",
      "2023-08-12 04:55:22.733166: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52cf151500 of size 307200000 next 48\n",
      "2023-08-12 04:55:22.733170: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1649500 of size 36864 next 63\n",
      "2023-08-12 04:55:22.733174: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652500 of size 256 next 120\n",
      "2023-08-12 04:55:22.733177: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652600 of size 256 next 107\n",
      "2023-08-12 04:55:22.733181: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652700 of size 256 next 116\n",
      "2023-08-12 04:55:22.733184: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652800 of size 256 next 97\n",
      "2023-08-12 04:55:22.733189: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652900 of size 256 next 113\n",
      "2023-08-12 04:55:22.733192: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652a00 of size 256 next 98\n",
      "2023-08-12 04:55:22.733196: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652b00 of size 256 next 96\n",
      "2023-08-12 04:55:22.733200: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652c00 of size 200192 next 104\n",
      "2023-08-12 04:55:22.733204: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683a00 of size 256 next 131\n",
      "2023-08-12 04:55:22.733208: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683b00 of size 256 next 115\n",
      "2023-08-12 04:55:22.733211: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683c00 of size 256 next 82\n",
      "2023-08-12 04:55:22.733215: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683d00 of size 256 next 122\n",
      "2023-08-12 04:55:22.733218: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683e00 of size 256 next 81\n",
      "2023-08-12 04:55:22.733223: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683f00 of size 256 next 119\n",
      "2023-08-12 04:55:22.733227: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e1684000 of size 2048 next 130\n",
      "2023-08-12 04:55:22.733230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1684800 of size 4513024 next 95\n",
      "2023-08-12 04:55:22.733236: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2500 of size 256 next 108\n",
      "2023-08-12 04:55:22.733240: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2600 of size 256 next 87\n",
      "2023-08-12 04:55:22.733243: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2700 of size 256 next 80\n",
      "2023-08-12 04:55:22.733247: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2800 of size 256 next 84\n",
      "2023-08-12 04:55:22.733251: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e1ad2900 of size 256 next 85\n",
      "2023-08-12 04:55:22.733255: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2a00 of size 61440000 next 90\n",
      "2023-08-12 04:55:22.733260: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e556aa00 of size 61440000 next 105\n",
      "2023-08-12 04:55:22.733264: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e9002a00 of size 110592 next 103\n",
      "2023-08-12 04:55:22.733268: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e901da00 of size 6191104 next 112\n",
      "2023-08-12 04:55:22.733272: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e9605200 of size 156809984 next 89\n",
      "2023-08-12 04:55:22.733277: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52f2b90d00 of size 256 next 101\n",
      "2023-08-12 04:55:22.733281: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52f2b90e00 of size 50331648 next 128\n",
      "2023-08-12 04:55:22.733286: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52f5b90e00 of size 33554432 next 127\n",
      "2023-08-12 04:55:22.733290: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52f7b90e00 of size 184414720 next 18446744073709551615\n",
      "2023-08-12 04:55:22.733294: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-08-12 04:55:22.733301: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 75 Chunks of size 256 totalling 18.8KiB\n",
      "2023-08-12 04:55:22.733306: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 512 totalling 1.0KiB\n",
      "2023-08-12 04:55:22.733311: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1280 totalling 3.8KiB\n",
      "2023-08-12 04:55:22.733315: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2048 totalling 4.0KiB\n",
      "2023-08-12 04:55:22.733320: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 10496 totalling 20.5KiB\n",
      "2023-08-12 04:55:22.733325: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 26368 totalling 25.8KiB\n",
      "2023-08-12 04:55:22.733330: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 32768 totalling 32.0KiB\n",
      "2023-08-12 04:55:22.733335: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 36864 totalling 180.0KiB\n",
      "2023-08-12 04:55:22.733339: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 40960 totalling 40.0KiB\n",
      "2023-08-12 04:55:22.733344: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 110592 totalling 324.0KiB\n",
      "2023-08-12 04:55:22.733348: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 200192 totalling 195.5KiB\n",
      "2023-08-12 04:55:22.733352: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1000192 totalling 976.8KiB\n",
      "2023-08-12 04:55:22.733356: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4194304 totalling 4.00MiB\n",
      "2023-08-12 04:55:22.733361: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4513024 totalling 4.30MiB\n",
      "2023-08-12 04:55:22.733365: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 7240960 totalling 6.91MiB\n",
      "2023-08-12 04:55:22.733369: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 33554432 totalling 32.00MiB\n",
      "2023-08-12 04:55:22.733375: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 61440000 totalling 117.19MiB\n",
      "2023-08-12 04:55:22.733379: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 156809984 totalling 149.54MiB\n",
      "2023-08-12 04:55:22.733383: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 184414720 totalling 175.87MiB\n",
      "2023-08-12 04:55:22.733388: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 307200000 totalling 585.94MiB\n",
      "2023-08-12 04:55:22.733392: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 1.05GiB\n",
      "2023-08-12 04:55:22.733397: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 1186398208 memory_limit_: 1186398208 available bytes: 0 curr_region_allocation_bytes_: 2372796416\n",
      "2023-08-12 04:55:22.733404: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      1186398208\n",
      "InUse:                      1129873152\n",
      "MaxInUse:                   1183362048\n",
      "NumAllocs:                       28086\n",
      "MaxAllocSize:                412200192\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-08-12 04:55:22.733413: W tensorflow/tsl/framework/bfc_allocator.cc:497] *************************************************************************xxxx*___************xxxxxxx\n",
      "2023-08-12 04:55:22.733430: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_grad_input_ops.cc:342 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[256,96,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-08-12 04:55:22.733461: I tensorflow/core/common_runtime/executor.cc:1209] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[256,96,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_131827/4078018487.py\", line 11, in <module>\n      acc1, acc2 = train_first_stage(lambda_val)\n    File \"/tmp/ipykernel_131827/3388643537.py\", line 71, in train_first_stage\n      history = model.fit(x=x_1train, y=(Y_1train,x_1train),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\", line 542, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\", line 275, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput'\nOOM when allocating tensor with shape[256,96,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1810]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_val \u001b[38;5;129;01min\u001b[39;00m lambda_list:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda: \u001b[39m\u001b[38;5;124m\"\u001b[39m,lambda_val)\n\u001b[0;32m---> 11\u001b[0m     acc1, acc2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_first_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     acc1_list\u001b[38;5;241m.\u001b[39mappend(acc1)\n\u001b[1;32m     13\u001b[0m     acc2_list\u001b[38;5;241m.\u001b[39mappend(acc2)\n",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m, in \u001b[0;36mtrain_first_stage\u001b[0;34m(lambda_val)\u001b[0m\n\u001b[1;32m     63\u001b[0m SNR \u001b[38;5;241m=\u001b[39m eb_n0\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#     print(\"current snr: \",SNR)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# #     print(\"====================\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     reporter = getMIOutput(trn=x_1train, \u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#                                tst=x_1test, \u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#                                snr=SNR,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#                               do_save_func=do_report)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_1train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mY_1train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_1train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_1test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_1test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_1test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m                    \u001b[38;5;66;03m#callbacks = [callback])\u001b[39;00m\n\u001b[1;32m     77\u001b[0m                     \u001b[38;5;66;03m#callbacks=[reporter,])\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_CE_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_131827/4078018487.py\", line 11, in <module>\n      acc1, acc2 = train_first_stage(lambda_val)\n    File \"/tmp/ipykernel_131827/3388643537.py\", line 71, in train_first_stage\n      history = model.fit(x=x_1train, y=(Y_1train,x_1train),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\", line 542, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py\", line 275, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput'\nOOM when allocating tensor with shape[256,96,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1810]"
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,0.2,0.5,1,3,5,10,15]\n",
    "acc1_list = []\n",
    "acc2_list = []\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    LABEL_SECOND_HALF = label2_list[5-i]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        acc1, acc2 = train_first_stage(lambda_val)\n",
    "        acc1_list.append(acc1)\n",
    "        acc2_list.append(acc2)\n",
    "        #train_second_stage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f522c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 08:24:50.058525: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 292.97MiB (rounded to 307200000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-08-12 08:24:50.058600: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-08-12 08:24:50.058610: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 68, Chunks in use: 68. 17.0KiB allocated for chunks. 17.0KiB in use in bin. 1.9KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058620: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 3, Chunks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.1KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058626: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 5, Chunks in use: 4. 6.8KiB allocated for chunks. 5.0KiB in use in bin. 4.4KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058633: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 3, Chunks in use: 3. 6.5KiB allocated for chunks. 6.5KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058639: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058645: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 3, Chunks in use: 3. 30.8KiB allocated for chunks. 30.8KiB in use in bin. 30.4KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058652: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 4, Chunks in use: 2. 82.2KiB allocated for chunks. 45.8KiB in use in bin. 40.0KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058659: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 11, Chunks in use: 10. 392.0KiB allocated for chunks. 360.0KiB in use in bin. 344.0KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058666: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 4, Chunks in use: 3. 396.0KiB allocated for chunks. 324.0KiB in use in bin. 324.0KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058673: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 1, Chunks in use: 1. 195.5KiB allocated for chunks. 195.5KiB in use in bin. 195.3KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058679: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058685: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 976.8KiB allocated for chunks. 976.8KiB in use in bin. 976.6KiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058691: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058697: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 0. 3.92MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058705: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 3, Chunks in use: 3. 14.91MiB allocated for chunks. 14.91MiB in use in bin. 12.00MiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058710: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 0. 8.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058716: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058723: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 2. 117.19MiB allocated for chunks. 117.19MiB in use in bin. 117.19MiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058728: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058736: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 0. 399.43MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058743: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 2. 585.94MiB allocated for chunks. 585.94MiB in use in bin. 585.94MiB client-requested in use in bin.\n",
      "2023-08-12 08:24:50.058749: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 292.97MiB was 256.00MiB, Chunk State: \n",
      "2023-08-12 08:24:50.058754: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 1186398208\n",
      "2023-08-12 08:24:50.058766: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000000 of size 1280 next 1\n",
      "2023-08-12 08:24:50.058772: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000500 of size 256 next 2\n",
      "2023-08-12 08:24:50.058777: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000600 of size 256 next 3\n",
      "2023-08-12 08:24:50.058782: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000700 of size 256 next 5\n",
      "2023-08-12 08:24:50.058787: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000800 of size 512 next 6\n",
      "2023-08-12 08:24:50.058792: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000a00 of size 256 next 4\n",
      "2023-08-12 08:24:50.058796: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000b00 of size 256 next 7\n",
      "2023-08-12 08:24:50.058801: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000c00 of size 256 next 12\n",
      "2023-08-12 08:24:50.058806: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000d00 of size 256 next 10\n",
      "2023-08-12 08:24:50.058811: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000e00 of size 256 next 11\n",
      "2023-08-12 08:24:50.058815: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc000f00 of size 256 next 17\n",
      "2023-08-12 08:24:50.058819: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001000 of size 256 next 15\n",
      "2023-08-12 08:24:50.058824: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001100 of size 256 next 16\n",
      "2023-08-12 08:24:50.058829: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001200 of size 2048 next 22\n",
      "2023-08-12 08:24:50.058833: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001a00 of size 256 next 20\n",
      "2023-08-12 08:24:50.058838: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001b00 of size 256 next 21\n",
      "2023-08-12 08:24:50.058842: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001c00 of size 256 next 27\n",
      "2023-08-12 08:24:50.058847: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001d00 of size 256 next 25\n",
      "2023-08-12 08:24:50.058851: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001e00 of size 256 next 26\n",
      "2023-08-12 08:24:50.058855: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc001f00 of size 256 next 31\n",
      "2023-08-12 08:24:50.058860: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002000 of size 256 next 32\n",
      "2023-08-12 08:24:50.058864: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002100 of size 256 next 34\n",
      "2023-08-12 08:24:50.058869: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002200 of size 256 next 35\n",
      "2023-08-12 08:24:50.058874: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002300 of size 256 next 33\n",
      "2023-08-12 08:24:50.058878: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002400 of size 256 next 36\n",
      "2023-08-12 08:24:50.058883: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002500 of size 256 next 39\n",
      "2023-08-12 08:24:50.058887: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002600 of size 256 next 40\n",
      "2023-08-12 08:24:50.058893: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002700 of size 256 next 41\n",
      "2023-08-12 08:24:50.058898: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002800 of size 256 next 42\n",
      "2023-08-12 08:24:50.058903: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002900 of size 256 next 43\n",
      "2023-08-12 08:24:50.058907: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002a00 of size 256 next 44\n",
      "2023-08-12 08:24:50.058911: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002b00 of size 256 next 45\n",
      "2023-08-12 08:24:50.058916: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002c00 of size 256 next 49\n",
      "2023-08-12 08:24:50.058920: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002d00 of size 256 next 37\n",
      "2023-08-12 08:24:50.058925: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc002e00 of size 1280 next 38\n",
      "2023-08-12 08:24:50.058930: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003300 of size 256 next 50\n",
      "2023-08-12 08:24:50.058934: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003400 of size 256 next 51\n",
      "2023-08-12 08:24:50.058940: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003500 of size 256 next 52\n",
      "2023-08-12 08:24:50.058946: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003600 of size 256 next 53\n",
      "2023-08-12 08:24:50.058951: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003700 of size 256 next 54\n",
      "2023-08-12 08:24:50.058956: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003800 of size 512 next 56\n",
      "2023-08-12 08:24:50.058962: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003a00 of size 256 next 58\n",
      "2023-08-12 08:24:50.058967: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003b00 of size 256 next 59\n",
      "2023-08-12 08:24:50.058972: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003c00 of size 256 next 61\n",
      "2023-08-12 08:24:50.058976: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc003d00 of size 2048 next 62\n",
      "2023-08-12 08:24:50.058981: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004500 of size 256 next 64\n",
      "2023-08-12 08:24:50.058986: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004600 of size 256 next 65\n",
      "2023-08-12 08:24:50.058990: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004700 of size 1280 next 66\n",
      "2023-08-12 08:24:50.058995: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004c00 of size 256 next 67\n",
      "2023-08-12 08:24:50.058999: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004d00 of size 256 next 68\n",
      "2023-08-12 08:24:50.059004: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004e00 of size 256 next 69\n",
      "2023-08-12 08:24:50.059009: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc004f00 of size 2560 next 79\n",
      "2023-08-12 08:24:50.059013: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005900 of size 256 next 109\n",
      "2023-08-12 08:24:50.059017: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005a00 of size 256 next 88\n",
      "2023-08-12 08:24:50.059022: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005b00 of size 256 next 8\n",
      "2023-08-12 08:24:50.059027: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc005c00 of size 10496 next 9\n",
      "2023-08-12 08:24:50.059032: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc008500 of size 40960 next 29\n",
      "2023-08-12 08:24:50.059036: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc012500 of size 32768 next 19\n",
      "2023-08-12 08:24:50.059041: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc01a500 of size 36864 next 18\n",
      "2023-08-12 08:24:50.059046: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc023500 of size 10496 next 55\n",
      "2023-08-12 08:24:50.059050: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc025e00 of size 26368 next 30\n",
      "2023-08-12 08:24:50.059056: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc02c500 of size 36864 next 28\n",
      "2023-08-12 08:24:50.059060: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc035500 of size 36864 next 14\n",
      "2023-08-12 08:24:50.059065: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc03e500 of size 110592 next 13\n",
      "2023-08-12 08:24:50.059070: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc059500 of size 1000192 next 47\n",
      "2023-08-12 08:24:50.059075: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc14d800 of size 110592 next 57\n",
      "2023-08-12 08:24:50.059079: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc168800 of size 36864 next 60\n",
      "2023-08-12 08:24:50.059084: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc171800 of size 7240960 next 24\n",
      "2023-08-12 08:24:50.059088: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bc859500 of size 4194304 next 23\n",
      "2023-08-12 08:24:50.059094: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52bcc59500 of size 307200000 next 46\n",
      "2023-08-12 08:24:50.059098: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52cf151500 of size 307200000 next 48\n",
      "2023-08-12 08:24:50.059103: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1649500 of size 36864 next 63\n",
      "2023-08-12 08:24:50.059107: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652500 of size 256 next 120\n",
      "2023-08-12 08:24:50.059112: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652600 of size 256 next 107\n",
      "2023-08-12 08:24:50.059116: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652700 of size 256 next 116\n",
      "2023-08-12 08:24:50.059121: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652800 of size 256 next 97\n",
      "2023-08-12 08:24:50.059125: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652900 of size 256 next 113\n",
      "2023-08-12 08:24:50.059130: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652a00 of size 256 next 98\n",
      "2023-08-12 08:24:50.059134: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652b00 of size 256 next 96\n",
      "2023-08-12 08:24:50.059139: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1652c00 of size 200192 next 104\n",
      "2023-08-12 08:24:50.059143: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683a00 of size 512 next 115\n",
      "2023-08-12 08:24:50.059148: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683c00 of size 256 next 82\n",
      "2023-08-12 08:24:50.059152: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683d00 of size 256 next 122\n",
      "2023-08-12 08:24:50.059157: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683e00 of size 256 next 81\n",
      "2023-08-12 08:24:50.059161: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1683f00 of size 256 next 119\n",
      "2023-08-12 08:24:50.059166: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e1684000 of size 20992 next 77\n",
      "2023-08-12 08:24:50.059170: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1689200 of size 10496 next 76\n",
      "2023-08-12 08:24:50.059175: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e168bb00 of size 256 next 130\n",
      "2023-08-12 08:24:50.059179: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e168bc00 of size 256 next 128\n",
      "2023-08-12 08:24:50.059183: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e168bd00 of size 256 next 86\n",
      "2023-08-12 08:24:50.059188: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e168be00 of size 256 next 135\n",
      "2023-08-12 08:24:50.059192: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e168bf00 of size 1792 next 127\n",
      "2023-08-12 08:24:50.059197: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e168c600 of size 1280 next 133\n",
      "2023-08-12 08:24:50.059201: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e168cb00 of size 16384 next 78\n",
      "2023-08-12 08:24:50.059207: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1690b00 of size 20480 next 112\n",
      "2023-08-12 08:24:50.059212: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e1695b00 of size 32768 next 73\n",
      "2023-08-12 08:24:50.059216: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e169db00 of size 36864 next 72\n",
      "2023-08-12 08:24:50.059221: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e16a6b00 of size 73728 next 131\n",
      "2023-08-12 08:24:50.059225: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e16b8b00 of size 36864 next 75\n",
      "2023-08-12 08:24:50.059230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e16c1b00 of size 110592 next 74\n",
      "2023-08-12 08:24:50.059234: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e16dcb00 of size 36864 next 103\n",
      "2023-08-12 08:24:50.059239: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e16e5b00 of size 4114944 next 95\n",
      "2023-08-12 08:24:50.059244: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2500 of size 256 next 108\n",
      "2023-08-12 08:24:50.059249: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2600 of size 256 next 87\n",
      "2023-08-12 08:24:50.059254: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2700 of size 256 next 80\n",
      "2023-08-12 08:24:50.059258: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2800 of size 256 next 84\n",
      "2023-08-12 08:24:50.059263: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2900 of size 256 next 85\n",
      "2023-08-12 08:24:50.059268: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e1ad2a00 of size 61440000 next 90\n",
      "2023-08-12 08:24:50.059273: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e556aa00 of size 61440000 next 105\n",
      "2023-08-12 08:24:50.059277: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e9002a00 of size 8388608 next 71\n",
      "2023-08-12 08:24:50.059282: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52e9802a00 of size 4194304 next 70\n",
      "2023-08-12 08:24:50.059287: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52e9c02a00 of size 150528768 next 89\n",
      "2023-08-12 08:24:50.059291: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f52f2b90d00 of size 256 next 101\n",
      "2023-08-12 08:24:50.059296: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f52f2b90e00 of size 268300800 next 18446744073709551615\n",
      "2023-08-12 08:24:50.059301: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-08-12 08:24:50.059308: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 68 Chunks of size 256 totalling 17.0KiB\n",
      "2023-08-12 08:24:50.059314: I tensorf"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_val \u001b[38;5;129;01min\u001b[39;00m lambda_list:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda: \u001b[39m\u001b[38;5;124m\"\u001b[39m,lambda_val)\n\u001b[0;32m---> 11\u001b[0m     acc1, acc2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_first_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     acc1_list\u001b[38;5;241m.\u001b[39mappend(acc1)\n\u001b[1;32m     13\u001b[0m     acc2_list\u001b[38;5;241m.\u001b[39mappend(acc2)\n",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m, in \u001b[0;36mtrain_first_stage\u001b[0;34m(lambda_val)\u001b[0m\n\u001b[1;32m     63\u001b[0m SNR \u001b[38;5;241m=\u001b[39m eb_n0\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#     print(\"current snr: \",SNR)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# #     print(\"====================\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     reporter = getMIOutput(trn=x_1train, \u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#                                tst=x_1test, \u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#                                snr=SNR,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#                               do_save_func=do_report)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_1train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mY_1train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_1train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_1test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_1test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_1test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m                    \u001b[38;5;66;03m#callbacks = [callback])\u001b[39;00m\n\u001b[1;32m     77\u001b[0m                     \u001b[38;5;66;03m#callbacks=[reporter,])\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_CE_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "low/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 512 totalling 1.5KiB\n",
      "2023-08-12 08:24:50.059319: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 1280 totalling 5.0KiB\n",
      "2023-08-12 08:24:50.059324: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2048 totalling 4.0KiB\n",
      "2023-08-12 08:24:50.059330: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2560 totalling 2.5KiB\n",
      "2023-08-12 08:24:50.059335: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 10496 totalling 30.8KiB\n",
      "2023-08-12 08:24:50.059340: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 20480 totalling 20.0KiB\n",
      "2023-08-12 08:24:50.059346: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 26368 totalling 25.8KiB\n",
      "2023-08-12 08:24:50.059351: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 32768 totalling 32.0KiB\n",
      "2023-08-12 08:24:50.059356: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 8 Chunks of size 36864 totalling 288.0KiB\n",
      "2023-08-12 08:24:50.059361: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 40960 totalling 40.0KiB\n",
      "2023-08-12 08:24:50.059367: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 110592 totalling 324.0KiB\n",
      "2023-08-12 08:24:50.059373: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 200192 totalling 195.5KiB\n",
      "2023-08-12 08:24:50.059378: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1000192 totalling 976.8KiB\n",
      "2023-08-12 08:24:50.059383: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 4194304 totalling 8.00MiB\n",
      "2023-08-12 08:24:50.059388: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 7240960 totalling 6.91MiB\n",
      "2023-08-12 08:24:50.059393: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 61440000 totalling 117.19MiB\n",
      "2023-08-12 08:24:50.059398: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 307200000 totalling 585.94MiB\n",
      "2023-08-12 08:24:50.059403: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 719.95MiB\n",
      "2023-08-12 08:24:50.059409: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 1186398208 memory_limit_: 1186398208 available bytes: 0 curr_region_allocation_bytes_: 2372796416\n",
      "2023-08-12 08:24:50.059419: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      1186398208\n",
      "InUse:                       754919424\n",
      "MaxInUse:                   1183362048\n",
      "NumAllocs:                       28158\n",
      "MaxAllocSize:                412200192\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-08-12 08:24:50.059429: W tensorflow/tsl/framework/bfc_allocator.cc:497] *****************************************************************____________*______________________\n"
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,0.2,0.5,1,3,5,10,15]\n",
    "acc1_list = []\n",
    "acc2_list = []\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    LABEL_SECOND_HALF = label2_list[5-i]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        acc1, acc2 = train_first_stage(lambda_val)\n",
    "        acc1_list.append(acc1)\n",
    "        acc2_list.append(acc2)\n",
    "        #train_second_stage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988eb60a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc1_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43macc1_list\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc1_list' is not defined"
     ]
    }
   ],
   "source": [
    "acc1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa606a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  5\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 08:26:59.207227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78836 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2023-08-12 08:27:02.458515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2023-08-12 08:27:02.697554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:655] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-12 08:27:02.786255: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x55ef60963f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-12 08:27:02.786299: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2023-08-12 08:27:02.850942: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7545999884605408\n",
      "stage 2!\n",
      "acc: 0.7680000066757202\n",
      "lambda:  0.2\n",
      "acc: 0.7680000066757202\n",
      "stage 2!\n",
      "acc: 0.7738000154495239\n",
      "lambda:  0.5\n",
      "acc: 0.7605999708175659\n",
      "stage 2!\n",
      "acc: 0.7688000202178955\n",
      "lambda:  1\n",
      "acc: 0.748199999332428\n",
      "stage 2!\n",
      "acc: 0.7626000046730042\n",
      "lambda:  3\n",
      "acc: 0.7738000154495239\n",
      "stage 2!\n",
      "acc: 0.7835999727249146\n",
      "lambda:  5\n",
      "acc: 0.7680000066757202\n",
      "stage 2!\n",
      "acc: 0.777999997138977\n",
      "lambda:  10\n",
      "acc: 0.7703999876976013\n",
      "stage 2!\n",
      "acc: 0.7788000106811523\n",
      "lambda:  15\n",
      "acc: 0.7767999768257141\n",
      "stage 2!\n",
      "acc: 0.7857999801635742\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.7581999897956848\n",
      "stage 2!\n",
      "acc: 0.7694000005722046\n",
      "lambda:  0.2\n",
      "acc: 0.7487999796867371\n",
      "stage 2!\n",
      "acc: 0.7656000256538391\n",
      "lambda:  0.5\n",
      "acc: 0.7563999891281128\n",
      "stage 2!\n",
      "acc: 0.7645999789237976\n",
      "lambda:  1\n",
      "acc: 0.7572000026702881\n",
      "stage 2!\n",
      "acc: 0.7742000222206116\n",
      "lambda:  3\n",
      "acc: 0.7752000093460083\n",
      "stage 2!\n",
      "acc: 0.7918000221252441\n",
      "lambda:  5\n",
      "acc: 0.769599974155426\n",
      "stage 2!\n",
      "acc: 0.7666000127792358\n",
      "lambda:  10\n",
      "acc: 0.7688000202178955\n",
      "stage 2!\n",
      "acc: 0.7793999910354614\n",
      "lambda:  15\n",
      "acc: 0.7670000195503235\n",
      "stage 2!\n",
      "acc: 0.776199996471405\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.7694000005722046\n",
      "stage 2!\n",
      "acc: 0.7702000141143799\n",
      "lambda:  0.2\n",
      "acc: 0.7476000189781189\n",
      "stage 2!\n",
      "acc: 0.756600022315979\n",
      "lambda:  0.5\n",
      "acc: 0.753600001335144\n",
      "stage 2!\n",
      "acc: 0.7657999992370605\n",
      "lambda:  1\n",
      "acc: 0.7472000122070312\n",
      "stage 2!\n",
      "acc: 0.7702000141143799\n",
      "lambda:  3\n",
      "acc: 0.7699999809265137\n",
      "stage 2!\n",
      "acc: 0.7824000120162964\n",
      "lambda:  5\n",
      "acc: 0.7639999985694885\n",
      "stage 2!\n",
      "acc: 0.7710000276565552\n",
      "lambda:  10\n",
      "acc: 0.7699999809265137\n",
      "stage 2!\n",
      "acc: 0.7760000228881836\n",
      "lambda:  15\n",
      "acc: 0.7720000147819519\n",
      "stage 2!\n",
      "acc: 0.7802000045776367\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.7675999999046326\n",
      "stage 2!\n",
      "acc: 0.76419997215271\n",
      "lambda:  0.2\n",
      "acc: 0.7590000033378601\n",
      "stage 2!\n",
      "acc: 0.7605999708175659\n",
      "lambda:  0.5\n",
      "acc: 0.7605999708175659\n",
      "stage 2!\n",
      "acc: 0.7742000222206116\n",
      "lambda:  1\n",
      "acc: 0.7598000168800354\n",
      "stage 2!\n",
      "acc: 0.7570000290870667\n",
      "lambda:  3\n",
      "acc: 0.7635999917984009\n",
      "stage 2!\n",
      "acc: 0.7649999856948853\n",
      "lambda:  5\n",
      "acc: 0.7603999972343445\n",
      "stage 2!\n",
      "acc: 0.7724000215530396\n",
      "lambda:  10\n",
      "acc: 0.7680000066757202\n",
      "stage 2!\n",
      "acc: 0.7721999883651733\n",
      "lambda:  15\n",
      "acc: 0.7620000243186951\n",
      "stage 2!\n",
      "acc: 0.7757999897003174\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.7440000176429749\n",
      "stage 2!\n",
      "acc: 0.76419997215271\n",
      "lambda:  0.2\n",
      "acc: 0.7495999932289124\n",
      "stage 2!\n",
      "acc: 0.7694000005722046\n",
      "lambda:  0.5\n",
      "acc: 0.7437999844551086\n",
      "stage 2!\n",
      "acc: 0.7562000155448914\n",
      "lambda:  1\n",
      "acc: 0.7609999775886536\n",
      "stage 2!\n",
      "acc: 0.7645999789237976\n",
      "lambda:  3\n",
      "acc: 0.7558000087738037\n",
      "stage 2!\n",
      "acc: 0.7685999870300293\n",
      "lambda:  5\n",
      "acc: 0.76419997215271\n",
      "stage 2!\n",
      "acc: 0.7760000228881836\n",
      "lambda:  10\n",
      "acc: 0.7591999769210815\n",
      "stage 2!\n",
      "acc: 0.771399974822998\n",
      "lambda:  15\n",
      "acc: 0.7652000188827515\n",
      "stage 2!\n",
      "acc: 0.7712000012397766\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.7486000061035156\n",
      "stage 2!\n",
      "acc: 0.758400022983551\n",
      "lambda:  0.2\n",
      "acc: 0.7626000046730042\n",
      "stage 2!\n",
      "acc: 0.7674000263214111\n",
      "lambda:  0.5\n",
      "acc: 0.7570000290870667\n",
      "stage 2!\n",
      "acc: 0.7670000195503235\n",
      "lambda:  1\n",
      "acc: 0.7623999714851379\n",
      "stage 2!\n",
      "acc: 0.7648000121116638\n",
      "lambda:  3\n",
      "acc: 0.7702000141143799\n",
      "stage 2!\n",
      "acc: 0.774399995803833\n",
      "lambda:  5\n",
      "acc: 0.7567999958992004\n",
      "stage 2!\n",
      "acc: 0.7649999856948853\n",
      "lambda:  10\n",
      "acc: 0.7644000053405762\n",
      "stage 2!\n",
      "acc: 0.7820000052452087\n",
      "lambda:  15\n",
      "acc: 0.7710000276565552\n",
      "stage 2!\n",
      "acc: 0.7656000256538391\n"
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,0.2,0.5,1,3,5,10,15]\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (5) )\n",
    "    LABEL_SECOND_HALF = label2_list[5]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        train_first_stage(lambda_val)\n",
    "        #train_second_stage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
