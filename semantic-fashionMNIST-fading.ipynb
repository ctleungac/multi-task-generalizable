{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d99a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import functions.getLayerOutput as glo\n",
    "\n",
    "import ssl\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebb564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a73d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 200\n",
    "\n",
    "# training SNR\n",
    "eb_n0 = 10\n",
    "channel_snr = 10\n",
    "rate = 1 # code rate \n",
    "channel_type = 'fading'\n",
    "\n",
    "\n",
    "#trn, tst = utils.get_mnist()\n",
    "\n",
    "# ##### KSG MI estimator #######################\n",
    "# settings = {'kraskov_k': 3}\n",
    "# gpu_est = est.OpenCLKraskovMI(settings = settings)\n",
    "# # usage: MI = estimator.estimate(var1, var2)\n",
    "\n",
    "##### setting for communication channel ######\n",
    "\n",
    "def getnoisevariance(SNR,rate,P=1):\n",
    "    # the SNR in args[0] is actually EbN0\n",
    "    snrdB = SNR + 10*np.log10(rate)\n",
    "    snr = 10.0**(snrdB/10.0)\n",
    "    #P_avg = 1\n",
    "    N0 = P/snr\n",
    "    return (N0/2)\n",
    "\n",
    "noise_var = getnoisevariance(eb_n0,rate,P=1)\n",
    "\n",
    "##############################################\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "label_permutation = np.arange(10)\n",
    "LABEL_FIRST_HALF = label_permutation[:5]\n",
    "label2_list = np.zeros((6,5))\n",
    "for i in range(6):\n",
    "    label2_list[i][:] = label_permutation[i:i+5]\n",
    "#LABEL_SECOND_HALF = [5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d2cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train = (x_train / 255.0)\n",
    "x_test = (x_test / 255.0)\n",
    "# reshape to 1d\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# save: original classification\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e58034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b848d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.where(np.in1d(y_train, LABEL_FIRST_HALF))\n",
    "test_filter =  np.where(np.in1d(y_test, LABEL_FIRST_HALF))\n",
    "\n",
    "x_1train = x_train[train_filter]\n",
    "Y_1train = Y_train[train_filter]\n",
    "x_1test = x_test[test_filter]\n",
    "Y_1test = Y_test[test_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7cb5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalized Input before \n",
    "# class GaussianNoiseLayer(layers.Layer):\n",
    "#     def __init__(self, stddev, **kwargs):\n",
    "#         super(GaussianNoiseLayer, self).__init__(**kwargs)\n",
    "#         self.stddev = stddev\n",
    "    \n",
    "#     def call(self, inputs, training=None):\n",
    "#         if training:\n",
    "#             noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=self.stddev)\n",
    "#             return inputs + noise\n",
    "#         else:\n",
    "#             noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=self.stddev)\n",
    "#             return inputs + noise\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super(GaussianNoiseLayer, self).get_config()\n",
    "#         config.update({'noise_var': self.stddev})\n",
    "#         return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc384bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fading channel\n",
    "\n",
    "def fading(x, stddev, h=None):\n",
    "    \"\"\"Implements the fading channel with multiplicative fading and\n",
    "    additive white gaussian noise.\n",
    "    Args:\n",
    "        x: channel input symbols\n",
    "        stddev: standard deviation of noise\n",
    "    Returns:\n",
    "        y: noisy channel output symbols\n",
    "    \"\"\"\n",
    "    # channel gain\n",
    "    if h is None:\n",
    "        h = tf.complex(\n",
    "            tf.random.normal([tf.shape(x)[0], 1], 0, 1 / np.sqrt(2)),\n",
    "            tf.random.normal([tf.shape(x)[0], 1], 0, 1 / np.sqrt(2)),\n",
    "        )\n",
    "\n",
    "    # additive white gaussian noise\n",
    "    awgn = tf.complex(\n",
    "        tf.random.normal(tf.shape(x), 0, 1 / np.sqrt(2)),\n",
    "        tf.random.normal(tf.shape(x), 0, 1 / np.sqrt(2)),\n",
    "    )\n",
    "\n",
    "    return (h * x + stddev * awgn), h\n",
    "\n",
    "class Channel(layers.Layer):\n",
    "\n",
    "#cite D-JSCC\n",
    "\n",
    "    def __init__(self, channel_type, channel_snr, name=\"channel\", **kwargs):\n",
    "        super(Channel, self).__init__(name=name, **kwargs)\n",
    "        self.channel_type = channel_type\n",
    "        self.channel_snr = channel_snr\n",
    "\n",
    "    def call(self, inputs):\n",
    "        (encoded_img, prev_h) = inputs\n",
    "        inter_shape = tf.shape(encoded_img)\n",
    "        # reshape array to [-1, dim_z]\n",
    "        z = layers.Flatten()(encoded_img)\n",
    "        # convert from snr to std\n",
    "        # print(\"channel_snr: {}\".format(self.channel_snr))\n",
    "        noise_stddev = np.sqrt(10 ** (-self.channel_snr / 10))\n",
    "\n",
    "        # Add channel noise\n",
    "        if self.channel_type == \"awgn\":\n",
    "            dim_z = tf.shape(z)[1]\n",
    "            # normalize latent vector so that the average power is 1\n",
    "            z_in = tf.sqrt(tf.cast(dim_z, dtype=tf.float32)) * tf.nn.l2_normalize(\n",
    "                z, axis=1\n",
    "            )\n",
    "            z_out = real_awgn(z_in, noise_stddev)\n",
    "            h = tf.ones_like(z_in)  # h just makes sense on fading channels\n",
    "\n",
    "        elif self.channel_type == \"fading\":\n",
    "            dim_z = tf.shape(z)[1] // 2\n",
    "            # convert z to complex representation\n",
    "            z_in = tf.complex(z[:, :dim_z], z[:, dim_z:])\n",
    "            # normalize the latent vector so that the average power is 1\n",
    "            z_norm = tf.reduce_sum(\n",
    "                tf.math.real(z_in * tf.math.conj(z_in)), axis=1, keepdims=True\n",
    "            )\n",
    "            z_in = z_in * tf.complex(\n",
    "                tf.sqrt(tf.cast(dim_z, dtype=tf.float32) / z_norm), 0.0\n",
    "            )\n",
    "            z_out, h = fading(z_in, noise_stddev, prev_h)\n",
    "            # convert back to real\n",
    "            z_out = tf.concat([tf.math.real(z_out), tf.math.imag(z_out)], 1)\n",
    "\n",
    "        # convert signal back to intermediate shape\n",
    "        z_out = tf.reshape(z_out, inter_shape)\n",
    "\n",
    "        return z_out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21aee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, display_frequency=10):\n",
    "        super(CustomMetricCallback, self).__init__()\n",
    "        self.display_frequency = display_frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.display_frequency == 0:\n",
    "            metrics_str = \" - \".join([f\"{metric_name}: {value:.4f}\" for metric_name, value in logs.items()])\n",
    "            print(f\"Epoch {epoch}/{self.params['epochs']} - {metrics_str}\")\n",
    "\n",
    "            \n",
    "callback = CustomMetricCallback(display_frequency=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610f52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUM = 1000\n",
    "EPOCHS = 2\n",
    "\n",
    "\n",
    "def do_report(epoch):\n",
    "    if epoch == (EPOCHS-1):\n",
    "        return True\n",
    "\n",
    "    \n",
    "    \n",
    "def train_first_stage(lambda_val):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    prev_chn_gain = None\n",
    "\n",
    "        ############ Model Structure ###############################\n",
    "\n",
    "    input_layer  = tf.keras.layers.Input((x_train.shape[1],))\n",
    "    encoder_1 = tf.keras.layers.Dense(128, activation='relu')(input_layer)\n",
    "    encoder_2 =  tf.keras.layers.Dense(80, activation='relu')(encoder_1)\n",
    "    ## first loss: classification \n",
    "    normalized_x = tf.keras.layers.Lambda(lambda x: K.tanh(x))(encoder_2)\n",
    "    fadingchannel = Channel(channel_type, channel_snr, name=\"channel_output\")\n",
    "    noise_layer, chn_gain = fadingchannel((normalized_x, prev_chn_gain))\n",
    "\n",
    "    CE_decoder_1 = tf.keras.layers.Dense(80, activation='relu')(noise_layer)\n",
    "    CE_decoder_2 = tf.keras.layers.Dense(80, activation='relu')(CE_decoder_1)\n",
    "    CE_output = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_decoder_2)\n",
    "\n",
    "    ## second loss: reconstruction\n",
    "    mse_decoder_1 = tf.keras.layers.Dense(256, activation='relu')(noise_layer)\n",
    "    mse_output = tf.keras.layers.Dense(x_train.shape[1],activation = 'sigmoid',name='mse')(mse_decoder_1) \n",
    "    \n",
    "    #    mse_decoder_1 = tf.keras.layers.Dense(256, activation='relu')(noise_layer)\n",
    "    #    mse_output = tf.keras.layers.Dense(x_train.shape[1],activation = 'sigmoid',name='mse')(mse_decoder_1)  \n",
    "\n",
    "    model = tf.keras.Model(inputs = input_layer, outputs = [CE_output,mse_output])\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.8)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss = {'CE' : 'categorical_crossentropy', \n",
    "                          'mse' : 'mse'},\n",
    "                  metrics = {'CE' : 'accuracy', \n",
    "                             'mse': tf.keras.metrics.RootMeanSquaredError()},\n",
    "                  loss_weights=[1, lambda_val])\n",
    "\n",
    "\n",
    "    SNR = eb_n0\n",
    "    #     print(\"====================\")\n",
    "    reporter = glo.getMIOutput(trn=[x_1train[:SAMPLE_NUM]], \n",
    "               num_selection = SAMPLE_NUM,\n",
    "               embedding_network = model,\n",
    "               target_layer = 2,\n",
    "               do_save_func=do_report)\n",
    "\n",
    "    history = model.fit(x=x_1train, y=(Y_1train,x_1train),\n",
    "                        batch_size=128,\n",
    "                        epochs=2,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_1test, (Y_1test,x_1test)),\n",
    "                        callbacks=[reporter,])\n",
    "    print(\"acc:\", history.history['val_CE_accuracy'][-1]) \n",
    "    \n",
    "    Z = reporter.datalist[-1]['data']['activity_tst'][-1] ## change 99 to -1\n",
    "    current_iteration = {\n",
    "    \"data\": Z\n",
    "        }\n",
    "\n",
    "\n",
    "#     ep_last = len(reporter.datalist) -1\n",
    "#     # Z_data = reporter.datalist[EPOCH]['data']['activity_tst'][3][:10000]\n",
    "#     ZHAT_data = reporter.datalist[ep_last]['data']['activity_tst'][4][:SAMPLE_NUM]\n",
    "#     x_selected = funcs.normalize_data_new(x_1train[:SAMPLE_NUM],C=10)[0]\n",
    "#     print(\"Current I(Z_hat; X) is: \",gpu_est.estimate(funcs.normalize_data_new(ZHAT_data,C=4)[0],x_selected))\n",
    "\n",
    "  \n",
    "    ################### secpmd stage #################\n",
    "    print(\"stage 2!\")\n",
    "   \n",
    "    train_filter = np.where(np.in1d(y_train, LABEL_SECOND_HALF))\n",
    "    test_filter =  np.where(np.in1d(y_test, LABEL_SECOND_HALF))\n",
    "\n",
    "    x_retrain = x_train[train_filter]\n",
    "    x_retest = x_test[test_filter]\n",
    "    Y_retrain = Y_train[train_filter]\n",
    "    Y_retest = Y_test[test_filter]\n",
    "\n",
    "    CE_dense_3 = layers.Dense(40, activation='relu')(noise_layer)\n",
    "    CE_dense_3 = layers.Dense(40, activation='relu')(CE_dense_3)\n",
    "    CE_output1 = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_dense_3)\n",
    "    #     CE_decoder_3 = tf.keras.layers.Dense(64, activation='relu')(noise_layer)\n",
    "    #     CE_decoder_4 = tf.keras.layers.Dense(64, activation='relu')(CE_decoder_3)\n",
    "    #     CE_output1 = tf.keras.layers.Dense(10, activation='softmax', name='CE')(CE_decoder_4)\n",
    "\n",
    "    reconstructed_model = tf.keras.models.Model(inputs = input_layer, outputs = [CE_output1,mse_output])\n",
    "\n",
    "    reconstructed_model.layers[1].trainable = False\n",
    "    reconstructed_model.layers[2].trainable = False\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=(5e-2)*2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.8)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "    reconstructed_model.compile(optimizer=opt, \n",
    "                loss = {'CE' : 'categorical_crossentropy', \n",
    "                          'mse' : 'mse'\n",
    "                         },              loss_weights=[1, 0],\n",
    "                  metrics = {'CE' : 'accuracy', \n",
    "                             'mse': tf.keras.metrics.RootMeanSquaredError()\n",
    "                           }\n",
    "                 )\n",
    "\n",
    "    #     reporter_2 = getMIOutput(trn=x_retrain, \n",
    "    #                                tst=Y_retrain, \n",
    "    #                                snr=SNR,\n",
    "    #                               do_save_func=do_report)\n",
    "\n",
    "    history1 = reconstructed_model.fit(x=x_retrain, y=(Y_retrain,x_retrain),\n",
    "                        batch_size=256,\n",
    "                        epochs=100,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_retest, (Y_retest,x_retest)))\n",
    "                        # callbacks=[callback])\n",
    "    print(\"acc:\", history1.history['val_CE_accuracy'][-1]) \n",
    "    return history.history['val_CE_accuracy'][-1], history1.history['val_CE_accuracy'][-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c069bc61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mZ\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d4bff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n",
      "Epoch 1/2\n",
      "240/240 [==============================] - 2s 5ms/step - loss: 1.0660 - CE_loss: 1.0660 - mse_loss: 0.2331 - CE_accuracy: 0.7078 - mse_root_mean_squared_error: 0.4828 - val_loss: 0.4691 - val_CE_loss: 0.4691 - val_mse_loss: 0.2332 - val_CE_accuracy: 0.9066 - val_mse_root_mean_squared_error: 0.4830\n",
      "Epoch 2/2\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.3523 - CE_loss: 0.3523 - mse_loss: 0.2331 - CE_accuracy: 0.9232 - mse_root_mean_squared_error: 0.4828 - val_loss: 0.2688 - val_CE_loss: 0.2688 - val_mse_loss: 0.2333 - val_CE_accuracy: 0.9401 - val_mse_root_mean_squared_error: 0.4830\n",
      "acc: 0.9400661587715149\n",
      "stage 2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7fe349feecb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/weakref.py\", line 370, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_val \u001b[38;5;129;01min\u001b[39;00m lambda_list:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda: \u001b[39m\u001b[38;5;124m\"\u001b[39m,lambda_val)\n\u001b[0;32m---> 11\u001b[0m     acc1, acc2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_first_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     acc1_list\u001b[38;5;241m.\u001b[39mappend(acc1)\n\u001b[1;32m     13\u001b[0m     acc2_list\u001b[38;5;241m.\u001b[39mappend(acc2)\n",
      "Cell \u001b[0;32mIn[16], line 125\u001b[0m, in \u001b[0;36mtrain_first_stage\u001b[0;34m(lambda_val)\u001b[0m\n\u001b[1;32m    111\u001b[0m reconstructed_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, \n\u001b[1;32m    112\u001b[0m             loss \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCE\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    113\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m                        }\n\u001b[1;32m    118\u001b[0m              )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#     reporter_2 = getMIOutput(trn=x_retrain, \u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#                                tst=Y_retrain, \u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#                                snr=SNR,\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#                               do_save_func=do_report)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m history1 \u001b[38;5;241m=\u001b[39m \u001b[43mreconstructed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_retrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mY_retrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_retrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_retest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_retest\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_retest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m                     \u001b[38;5;66;03m# callbacks=[callback])\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history1\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_CE_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,1,3,10]\n",
    "acc1_list = []\n",
    "acc2_list = []\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    LABEL_SECOND_HALF = label2_list[5-i]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        acc1, acc2 = train_first_stage(lambda_val)\n",
    "        acc1_list.append(acc1)\n",
    "        acc2_list.append(acc2)\n",
    "        #train_second_stage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5099a4f",
   "metadata": {},
   "source": [
    "## MI estiamtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18319bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUM = 10000\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def do_report(epoch):\n",
    "    if epoch == (EPOCHS-1):\n",
    "        return True\n",
    "\n",
    "    \n",
    "    \n",
    "def train_first_stage(lambda_val):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    prev_chn_gain = None\n",
    "\n",
    "        ############ Model Structure ###############################\n",
    "\n",
    "    input_layer  = tf.keras.layers.Input((x_train.shape[1],))\n",
    "    encoder_1 = tf.keras.layers.Dense(128, activation='relu')(input_layer)\n",
    "    encoder_2 =  tf.keras.layers.Dense(80, activation='relu')(encoder_1)\n",
    "    ## first loss: classification \n",
    "    normalized_x = tf.keras.layers.Lambda(lambda x: K.tanh(x))(encoder_2)\n",
    "    fadingchannel = Channel(channel_type, channel_snr, name=\"channel_output\")\n",
    "    noise_layer, chn_gain = fadingchannel((normalized_x, prev_chn_gain))\n",
    "\n",
    "    CE_decoder_1 = tf.keras.layers.Dense(80, activation='relu')(noise_layer)\n",
    "    CE_decoder_2 = tf.keras.layers.Dense(80, activation='relu')(CE_decoder_1)\n",
    "    CE_output = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_decoder_2)\n",
    "\n",
    "    ## second loss: reconstruction\n",
    "    mse_decoder_1 = tf.keras.layers.Dense(256, activation='relu')(noise_layer)\n",
    "    mse_output = tf.keras.layers.Dense(x_train.shape[1],activation = 'sigmoid',name='mse')(mse_decoder_1) \n",
    "    \n",
    "    #    mse_decoder_1 = tf.keras.layers.Dense(256, activation='relu')(noise_layer)\n",
    "    #    mse_output = tf.keras.layers.Dense(x_train.shape[1],activation = 'sigmoid',name='mse')(mse_decoder_1)  \n",
    "\n",
    "    model = tf.keras.Model(inputs = input_layer, outputs = [CE_output,mse_output])\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.8)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss = {'CE' : 'categorical_crossentropy', \n",
    "                          'mse' : 'mse'},\n",
    "                  metrics = {'CE' : 'accuracy', \n",
    "                             'mse': tf.keras.metrics.RootMeanSquaredError()},\n",
    "                  loss_weights=[1, lambda_val])\n",
    "\n",
    "\n",
    "    SNR = eb_n0\n",
    "    #     print(\"====================\")\n",
    "    reporter = glo.getMIOutput(trn=[x_1train[:SAMPLE_NUM]], \n",
    "               num_selection = SAMPLE_NUM,\n",
    "               embedding_network = model,\n",
    "               target_layer = 2,\n",
    "               do_save_func=do_report)\n",
    "\n",
    "    history = model.fit(x=x_1train, y=(Y_1train,x_1train),\n",
    "                        batch_size=128,\n",
    "                        epochs=EPOCHS,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_1test, (Y_1test,x_1test)),\n",
    "                        callbacks=[reporter,])\n",
    "    print(\"acc:\", history.history['val_CE_accuracy'][-1]) \n",
    "    \n",
    "    Z = reporter.datalist[-1]['data']['activity_tst'][-1] ## change 99 to -1\n",
    "#     current_iteration = {\n",
    "#     \"data\": Z\n",
    "#         }\n",
    "    return Z\n",
    "\n",
    "#     ep_last = len(reporter.datalist) -1\n",
    "#     # Z_data = reporter.datalist[EPOCH]['data']['activity_tst'][3][:10000]\n",
    "#     ZHAT_data = reporter.datalist[ep_last]['data']['activity_tst'][4][:SAMPLE_NUM]\n",
    "#     x_selected = funcs.normalize_data_new(x_1train[:SAMPLE_NUM],C=10)[0]\n",
    "#     print(\"Current I(Z_hat; X) is: \",gpu_est.estimate(funcs.normalize_data_new(ZHAT_data,C=4)[0],x_selected))\n",
    "\n",
    "  \n",
    "#     ################### secpmd stage #################\n",
    "#     print(\"stage 2!\")\n",
    "   \n",
    "#     train_filter = np.where(np.in1d(y_train, LABEL_SECOND_HALF))\n",
    "#     test_filter =  np.where(np.in1d(y_test, LABEL_SECOND_HALF))\n",
    "\n",
    "#     x_retrain = x_train[train_filter]\n",
    "#     x_retest = x_test[test_filter]\n",
    "#     Y_retrain = Y_train[train_filter]\n",
    "#     Y_retest = Y_test[test_filter]\n",
    "\n",
    "#     CE_dense_3 = layers.Dense(40, activation='relu')(noise_layer)\n",
    "#     CE_dense_3 = layers.Dense(40, activation='relu')(CE_dense_3)\n",
    "#     CE_output1 = tf.keras.layers.Dense(10,activation = 'softmax',name='CE')(CE_dense_3)\n",
    "#     #     CE_decoder_3 = tf.keras.layers.Dense(64, activation='relu')(noise_layer)\n",
    "#     #     CE_decoder_4 = tf.keras.layers.Dense(64, activation='relu')(CE_decoder_3)\n",
    "#     #     CE_output1 = tf.keras.layers.Dense(10, activation='softmax', name='CE')(CE_decoder_4)\n",
    "\n",
    "#     reconstructed_model = tf.keras.models.Model(inputs = input_layer, outputs = [CE_output1,mse_output])\n",
    "\n",
    "#     reconstructed_model.layers[1].trainable = False\n",
    "#     reconstructed_model.layers[2].trainable = False\n",
    "\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=(5e-2)*2,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.8)\n",
    "#     opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "#     reconstructed_model.compile(optimizer=opt, \n",
    "#                 loss = {'CE' : 'categorical_crossentropy', \n",
    "#                           'mse' : 'mse'\n",
    "#                          },              loss_weights=[1, 0],\n",
    "#                   metrics = {'CE' : 'accuracy', \n",
    "#                              'mse': tf.keras.metrics.RootMeanSquaredError()\n",
    "#                            }\n",
    "#                  )\n",
    "\n",
    "#     #     reporter_2 = getMIOutput(trn=x_retrain, \n",
    "#     #                                tst=Y_retrain, \n",
    "#     #                                snr=SNR,\n",
    "#     #                               do_save_func=do_report)\n",
    "\n",
    "#     history1 = reconstructed_model.fit(x=x_retrain, y=(Y_retrain,x_retrain),\n",
    "#                         batch_size=256,\n",
    "#                         epochs=100,\n",
    "#                         verbose=0,\n",
    "#                         validation_data=(x_retest, (Y_retest,x_retest)))\n",
    "#                         # callbacks=[callback])\n",
    "#     print(\"acc:\", history1.history['val_CE_accuracy'][-1]) \n",
    "#     return history.history['val_CE_accuracy'][-1], history1.history['val_CE_accuracy'][-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd37ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 07:33:02.062253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78835 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "2024-07-30 07:33:04.753966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:655] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-07-30 07:33:04.807757: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x5586db57f380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-30 07:33:04.807797: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-07-30 07:33:04.828527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8902\n",
      "2024-07-30 07:33:04.896253: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9025999903678894\n",
      "lambda:  1\n",
      "acc: 0.8877999782562256\n",
      "lambda:  3\n",
      "acc: 0.8889999985694885\n",
      "lambda:  10\n",
      "acc: 0.8971999883651733\n",
      "current overlap is:  1\n",
      "lambda:  0\n",
      "acc: 0.9028000235557556\n",
      "lambda:  1\n",
      "acc: 0.8934000134468079\n",
      "lambda:  3\n",
      "acc: 0.8873999714851379\n",
      "lambda:  10\n",
      "acc: 0.8978000283241272\n",
      "current overlap is:  2\n",
      "lambda:  0\n",
      "acc: 0.8949999809265137\n",
      "lambda:  1\n",
      "acc: 0.8949999809265137\n",
      "lambda:  3\n",
      "acc: 0.8885999917984009\n",
      "lambda:  10\n",
      "acc: 0.897599995136261\n",
      "current overlap is:  3\n",
      "lambda:  0\n",
      "acc: 0.8980000019073486\n",
      "lambda:  1\n",
      "acc: 0.8974000215530396\n",
      "lambda:  3\n",
      "acc: 0.8848000168800354\n",
      "lambda:  10\n",
      "acc: 0.8863999843597412\n",
      "current overlap is:  4\n",
      "lambda:  0\n",
      "acc: 0.8827999830245972\n",
      "lambda:  1\n",
      "acc: 0.8953999876976013\n",
      "lambda:  3\n",
      "acc: 0.8925999999046326\n",
      "lambda:  10\n",
      "acc: 0.8889999985694885\n",
      "current overlap is:  5\n",
      "lambda:  0\n",
      "acc: 0.9034000039100647\n",
      "lambda:  1\n",
      "acc: 0.8899999856948853\n",
      "lambda:  3\n",
      "acc: 0.8880000114440918\n",
      "lambda:  10\n",
      "acc: 0.8916000127792358\n"
     ]
    }
   ],
   "source": [
    "sd = np.sqrt(noise_var)\n",
    "\n",
    "lambda_list = [0,1,3,10]\n",
    "z_list = []\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    LABEL_SECOND_HALF = label2_list[5-i]\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \",lambda_val)\n",
    "        z = train_first_stage(lambda_val)\n",
    "        z_list.append(z)\n",
    "\n",
    "        #train_second_stage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3620a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.MIestimate import MINE_MI\n",
    "from functions.MIestimate import global_normalize_mine as VN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42230e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1390328",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_selected = VN(x_1train[:SAMPLE_NUM],C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c45aa433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "2024-07-30 08:48:23.220034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.4271302725162482\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.519193273453713\n",
      "lambda:  3\n",
      "I_Z_X:  3.6968056044098647\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.349830265076682\n",
      "current overlap is:  1\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.9886749534320254\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.81593351511361\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.8647628293465783\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.715101465521787\n",
      "current overlap is:  2\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.2007859722553516\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.421012849625058\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.9012726823329427\n",
      "lambda:  10\n",
      "I_Z_X:  4.6585023190881385\n",
      "current overlap is:  3\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.113853213793124\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.8597485031780225\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.9348815319645527\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.708338075793956\n",
      "current overlap is:  4\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.9634206770140286\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.1273748337692595\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.015169912693346\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  4.762281810098283\n",
      "current overlap is:  5\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.6173472119541352\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.9502935852389722\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  3.760189810385353\n",
      "lambda:  10\n",
      "I_Z_X:  4.648511846899519\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### TEST THE RANGE FOR PHASE 2\n",
    "\n",
    "izx_val_C_z=[]\n",
    "se = []\n",
    "\n",
    "count = 0\n",
    "#dnew=z_selected\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \", lambda_val)\n",
    "        z = z_list[count]\n",
    "        dnew = VN(z,C=1)\n",
    "\n",
    "        #dnew = z[:SAMPLE_NUM]\n",
    "        normz = VN(dnew,C=1)\n",
    "        izx_val,_ = MINE_MI(dnew,x_selected,total_epochs=50)\n",
    "\n",
    "        print(\"I_Z_X: \",izx_val)\n",
    "        #print(\"I_Z_Y: \",izy_val)\n",
    "        izx_val_C_z.append(izx_val)\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a530e6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.88520205, 3.78225943, 3.8621804 , 4.64042763])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.reshape(izx_val_C_z,(6,-1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d161f1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.42713027, 3.51919327, 3.6968056 , 4.34983027],\n",
       "       [3.98867495, 3.81593352, 3.86476283, 4.71510147],\n",
       "       [4.20078597, 3.42101285, 3.90127268, 4.65850232],\n",
       "       [4.11385321, 3.8597485 , 3.93488153, 4.70833808],\n",
       "       [3.96342068, 4.12737483, 4.01516991, 4.76228181],\n",
       "       [3.61734721, 3.95029359, 3.76018981, 4.64851185]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(izx_val_C_z,(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcc78df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mreshape(izx_val_C_z,(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "array([2.8692471 , 2.3776179 , 2.40168653, 3.04009828])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9fd6f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire list using pickle\n",
    "import pickle\n",
    "with open('z_mnist_fading.pkl', 'wb') as f:\n",
    "    pickle.dump(z_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5644fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_normalize_mine(data_orig, C):\n",
    "    # Reshape data as before\n",
    "    data = np.reshape(data_orig, (data_orig.shape[0], int(data_orig.size / data_orig.shape[0])))\n",
    "    \n",
    "    # Subtract the mean\n",
    "    means = np.mean(data, axis=0)\n",
    "    data = np.abs(data - means)\n",
    "    \n",
    "    # Compute the normalization factor\n",
    "    # norm = np.sqrt(np.mean(np.sum(data ** 2, axis=1)))\n",
    "    # linalg.norm(x, ord=None, \n",
    "    norm = np.sqrt(np.mean(np.sum(data ** 2, axis=1)))\n",
    "    norm = norm/ np.sqrt(data.shape[1])\n",
    "    \n",
    "    # print(norm)\n",
    "    # Normalize the data\n",
    "    \n",
    "    data = C*data_orig / norm\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6df0175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current overlap is:  0\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.148905161604618\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.6850456374786736\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.5967646667685054\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  2.3628347277260024\n",
      "current overlap is:  1\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.1108009434534303\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.610591295228111\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.5784631329769396\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  2.5759074735329537\n",
      "current overlap is:  2\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.102290897923437\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.7843455393774573\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.5132971867385128\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  2.587637034287099\n",
      "current overlap is:  3\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.8633876478267736\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.33282114841717764\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.5714655028786335\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  2.188694726490771\n",
      "current overlap is:  4\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.3481602193019728\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.33860509694834473\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.4207324264171555\n",
      "lambda:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  2.4810889665633646\n",
      "current overlap is:  5\n",
      "lambda:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.329271278766385\n",
      "lambda:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  0.5832660467240872\n",
      "lambda:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_Z_X:  1.6243355494936238\n",
      "lambda:  10\n",
      "I_Z_X:  2.482817048239234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### TEST THE RANGE FOR PHASE 2\n",
    "\n",
    "izx_val_C_z=[]\n",
    "se = []\n",
    "\n",
    "count = 0\n",
    "#dnew=z_selected\n",
    "for i in range(6):\n",
    "    print(\"current overlap is: \", (i) )\n",
    "    for lambda_val in lambda_list:\n",
    "        print(\"lambda: \", lambda_val)\n",
    "        z = z_list[count]\n",
    "        dnew = VN(z,C=1)[0]\n",
    "\n",
    "        #dnew = z[:SAMPLE_NUM]\n",
    "        \n",
    "        #normz,_ = VN(dnew,C=1)\n",
    "        izx_val,_ = MINE_MI(dnew,x_selected,total_epochs=50)\n",
    "\n",
    "        print(\"I_Z_X: \",izx_val)\n",
    "        #print(\"I_Z_Y: \",izy_val)\n",
    "        izx_val_C_z.append(izx_val)\n",
    "        count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e45e78bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'izx_val_C_z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mizx_val_C_z\u001b[49m,(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'izx_val_C_z' is not defined"
     ]
    }
   ],
   "source": [
    "np.reshape(izx_val_C_z,(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b46771fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14890516, 0.68504564, 1.59676467, 2.36283473],\n",
       "       [1.11080094, 0.6105913 , 1.57846313, 2.57590747],\n",
       "       [1.1022909 , 0.78434554, 1.51329719, 2.58763703],\n",
       "       [0.86338765, 0.33282115, 1.5714655 , 2.18869473],\n",
       "       [1.34816022, 0.3386051 , 1.42073243, 2.48108897],\n",
       "       [1.32927128, 0.58326605, 1.62433555, 2.48281705]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(izx_val_C_z,(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ca4989f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.15046936, 0.55577913, 1.55084308, 2.44649666])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.reshape(izx_val_C_z,(6,-1)),axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
